{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree Capstone Project\n",
    "## Forest Type Prediction\n",
    "## Bin Luo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Supress unnecessary warnings so that presentation looks clean\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#import data\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"../data_tree/train.csv\")\n",
    "\n",
    "#drop the ID columns\n",
    "dataset = dataset.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15120, 55)\n"
     ]
    }
   ],
   "source": [
    "#data description - shape\n",
    "r, c = dataset.shape\n",
    "print (r,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Wilderness_Area1</th>\n",
       "      <th>Wilderness_Area2</th>\n",
       "      <th>Wilderness_Area3</th>\n",
       "      <th>Wilderness_Area4</th>\n",
       "      <th>Soil_Type1</th>\n",
       "      <th>Soil_Type2</th>\n",
       "      <th>Soil_Type3</th>\n",
       "      <th>Soil_Type4</th>\n",
       "      <th>Soil_Type5</th>\n",
       "      <th>Soil_Type6</th>\n",
       "      <th>Soil_Type7</th>\n",
       "      <th>Soil_Type8</th>\n",
       "      <th>Soil_Type9</th>\n",
       "      <th>Soil_Type10</th>\n",
       "      <th>Soil_Type11</th>\n",
       "      <th>Soil_Type12</th>\n",
       "      <th>Soil_Type13</th>\n",
       "      <th>Soil_Type14</th>\n",
       "      <th>Soil_Type15</th>\n",
       "      <th>Soil_Type16</th>\n",
       "      <th>Soil_Type17</th>\n",
       "      <th>Soil_Type18</th>\n",
       "      <th>Soil_Type19</th>\n",
       "      <th>Soil_Type20</th>\n",
       "      <th>Soil_Type21</th>\n",
       "      <th>Soil_Type22</th>\n",
       "      <th>Soil_Type23</th>\n",
       "      <th>Soil_Type24</th>\n",
       "      <th>Soil_Type25</th>\n",
       "      <th>Soil_Type26</th>\n",
       "      <th>Soil_Type27</th>\n",
       "      <th>Soil_Type28</th>\n",
       "      <th>Soil_Type29</th>\n",
       "      <th>Soil_Type30</th>\n",
       "      <th>Soil_Type31</th>\n",
       "      <th>Soil_Type32</th>\n",
       "      <th>Soil_Type33</th>\n",
       "      <th>Soil_Type34</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.0</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.0</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2749.322553</td>\n",
       "      <td>156.676653</td>\n",
       "      <td>16.501587</td>\n",
       "      <td>227.195701</td>\n",
       "      <td>51.076521</td>\n",
       "      <td>1714.023214</td>\n",
       "      <td>212.704299</td>\n",
       "      <td>218.965608</td>\n",
       "      <td>135.091997</td>\n",
       "      <td>1511.147288</td>\n",
       "      <td>0.237897</td>\n",
       "      <td>0.033003</td>\n",
       "      <td>0.419907</td>\n",
       "      <td>0.309193</td>\n",
       "      <td>0.023479</td>\n",
       "      <td>0.041204</td>\n",
       "      <td>0.063624</td>\n",
       "      <td>0.055754</td>\n",
       "      <td>0.010913</td>\n",
       "      <td>0.042989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.141667</td>\n",
       "      <td>0.026852</td>\n",
       "      <td>0.015013</td>\n",
       "      <td>0.031481</td>\n",
       "      <td>0.011177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007540</td>\n",
       "      <td>0.040476</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0.009193</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.022817</td>\n",
       "      <td>0.050066</td>\n",
       "      <td>0.016997</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.085384</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.021958</td>\n",
       "      <td>0.045635</td>\n",
       "      <td>0.040741</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.006746</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.048148</td>\n",
       "      <td>0.043452</td>\n",
       "      <td>0.030357</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>417.678187</td>\n",
       "      <td>110.085801</td>\n",
       "      <td>8.453927</td>\n",
       "      <td>210.075296</td>\n",
       "      <td>61.239406</td>\n",
       "      <td>1325.066358</td>\n",
       "      <td>30.561287</td>\n",
       "      <td>22.801966</td>\n",
       "      <td>45.895189</td>\n",
       "      <td>1099.936493</td>\n",
       "      <td>0.425810</td>\n",
       "      <td>0.178649</td>\n",
       "      <td>0.493560</td>\n",
       "      <td>0.462176</td>\n",
       "      <td>0.151424</td>\n",
       "      <td>0.198768</td>\n",
       "      <td>0.244091</td>\n",
       "      <td>0.229454</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.202840</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008133</td>\n",
       "      <td>0.025710</td>\n",
       "      <td>0.348719</td>\n",
       "      <td>0.161656</td>\n",
       "      <td>0.121609</td>\n",
       "      <td>0.174621</td>\n",
       "      <td>0.105133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086506</td>\n",
       "      <td>0.197080</td>\n",
       "      <td>0.062871</td>\n",
       "      <td>0.055075</td>\n",
       "      <td>0.095442</td>\n",
       "      <td>0.032514</td>\n",
       "      <td>0.149326</td>\n",
       "      <td>0.218089</td>\n",
       "      <td>0.129265</td>\n",
       "      <td>0.008133</td>\n",
       "      <td>0.059657</td>\n",
       "      <td>0.031482</td>\n",
       "      <td>0.024391</td>\n",
       "      <td>0.279461</td>\n",
       "      <td>0.213667</td>\n",
       "      <td>0.146550</td>\n",
       "      <td>0.208699</td>\n",
       "      <td>0.197696</td>\n",
       "      <td>0.038118</td>\n",
       "      <td>0.081859</td>\n",
       "      <td>0.025710</td>\n",
       "      <td>0.047368</td>\n",
       "      <td>0.214086</td>\n",
       "      <td>0.203880</td>\n",
       "      <td>0.171574</td>\n",
       "      <td>2.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1863.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-146.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2376.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>764.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2752.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1316.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>1256.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3104.000000</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>2270.000000</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>1988.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3849.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1343.000000</td>\n",
       "      <td>554.000000</td>\n",
       "      <td>6890.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>6993.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Elevation        Aspect         Slope  \\\n",
       "count  15120.000000  15120.000000  15120.000000   \n",
       "mean    2749.322553    156.676653     16.501587   \n",
       "std      417.678187    110.085801      8.453927   \n",
       "min     1863.000000      0.000000      0.000000   \n",
       "25%     2376.000000     65.000000     10.000000   \n",
       "50%     2752.000000    126.000000     15.000000   \n",
       "75%     3104.000000    261.000000     22.000000   \n",
       "max     3849.000000    360.000000     52.000000   \n",
       "\n",
       "       Horizontal_Distance_To_Hydrology  Vertical_Distance_To_Hydrology  \\\n",
       "count                      15120.000000                    15120.000000   \n",
       "mean                         227.195701                       51.076521   \n",
       "std                          210.075296                       61.239406   \n",
       "min                            0.000000                     -146.000000   \n",
       "25%                           67.000000                        5.000000   \n",
       "50%                          180.000000                       32.000000   \n",
       "75%                          330.000000                       79.000000   \n",
       "max                         1343.000000                      554.000000   \n",
       "\n",
       "       Horizontal_Distance_To_Roadways  Hillshade_9am  Hillshade_Noon  \\\n",
       "count                     15120.000000   15120.000000    15120.000000   \n",
       "mean                       1714.023214     212.704299      218.965608   \n",
       "std                        1325.066358      30.561287       22.801966   \n",
       "min                           0.000000       0.000000       99.000000   \n",
       "25%                         764.000000     196.000000      207.000000   \n",
       "50%                        1316.000000     220.000000      223.000000   \n",
       "75%                        2270.000000     235.000000      235.000000   \n",
       "max                        6890.000000     254.000000      254.000000   \n",
       "\n",
       "       Hillshade_3pm  Horizontal_Distance_To_Fire_Points  Wilderness_Area1  \\\n",
       "count   15120.000000                        15120.000000      15120.000000   \n",
       "mean      135.091997                         1511.147288          0.237897   \n",
       "std        45.895189                         1099.936493          0.425810   \n",
       "min         0.000000                            0.000000          0.000000   \n",
       "25%       106.000000                          730.000000          0.000000   \n",
       "50%       138.000000                         1256.000000          0.000000   \n",
       "75%       167.000000                         1988.250000          0.000000   \n",
       "max       248.000000                         6993.000000          1.000000   \n",
       "\n",
       "       Wilderness_Area2  Wilderness_Area3  Wilderness_Area4    Soil_Type1  \\\n",
       "count      15120.000000      15120.000000      15120.000000  15120.000000   \n",
       "mean           0.033003          0.419907          0.309193      0.023479   \n",
       "std            0.178649          0.493560          0.462176      0.151424   \n",
       "min            0.000000          0.000000          0.000000      0.000000   \n",
       "25%            0.000000          0.000000          0.000000      0.000000   \n",
       "50%            0.000000          0.000000          0.000000      0.000000   \n",
       "75%            0.000000          1.000000          1.000000      0.000000   \n",
       "max            1.000000          1.000000          1.000000      1.000000   \n",
       "\n",
       "         Soil_Type2    Soil_Type3    Soil_Type4    Soil_Type5    Soil_Type6  \\\n",
       "count  15120.000000  15120.000000  15120.000000  15120.000000  15120.000000   \n",
       "mean       0.041204      0.063624      0.055754      0.010913      0.042989   \n",
       "std        0.198768      0.244091      0.229454      0.103896      0.202840   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       Soil_Type7    Soil_Type8    Soil_Type9   Soil_Type10   Soil_Type11  \\\n",
       "count     15120.0  15120.000000  15120.000000  15120.000000  15120.000000   \n",
       "mean          0.0      0.000066      0.000661      0.141667      0.026852   \n",
       "std           0.0      0.008133      0.025710      0.348719      0.161656   \n",
       "min           0.0      0.000000      0.000000      0.000000      0.000000   \n",
       "25%           0.0      0.000000      0.000000      0.000000      0.000000   \n",
       "50%           0.0      0.000000      0.000000      0.000000      0.000000   \n",
       "75%           0.0      0.000000      0.000000      0.000000      0.000000   \n",
       "max           0.0      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "        Soil_Type12   Soil_Type13   Soil_Type14  Soil_Type15   Soil_Type16  \\\n",
       "count  15120.000000  15120.000000  15120.000000      15120.0  15120.000000   \n",
       "mean       0.015013      0.031481      0.011177          0.0      0.007540   \n",
       "std        0.121609      0.174621      0.105133          0.0      0.086506   \n",
       "min        0.000000      0.000000      0.000000          0.0      0.000000   \n",
       "25%        0.000000      0.000000      0.000000          0.0      0.000000   \n",
       "50%        0.000000      0.000000      0.000000          0.0      0.000000   \n",
       "75%        0.000000      0.000000      0.000000          0.0      0.000000   \n",
       "max        1.000000      1.000000      1.000000          0.0      1.000000   \n",
       "\n",
       "        Soil_Type17   Soil_Type18   Soil_Type19   Soil_Type20   Soil_Type21  \\\n",
       "count  15120.000000  15120.000000  15120.000000  15120.000000  15120.000000   \n",
       "mean       0.040476      0.003968      0.003042      0.009193      0.001058   \n",
       "std        0.197080      0.062871      0.055075      0.095442      0.032514   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "        Soil_Type22   Soil_Type23   Soil_Type24   Soil_Type25   Soil_Type26  \\\n",
       "count  15120.000000  15120.000000  15120.000000  15120.000000  15120.000000   \n",
       "mean       0.022817      0.050066      0.016997      0.000066      0.003571   \n",
       "std        0.149326      0.218089      0.129265      0.008133      0.059657   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "        Soil_Type27   Soil_Type28   Soil_Type29   Soil_Type30   Soil_Type31  \\\n",
       "count  15120.000000  15120.000000  15120.000000  15120.000000  15120.000000   \n",
       "mean       0.000992      0.000595      0.085384      0.047950      0.021958   \n",
       "std        0.031482      0.024391      0.279461      0.213667      0.146550   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "        Soil_Type32   Soil_Type33   Soil_Type34   Soil_Type35   Soil_Type36  \\\n",
       "count  15120.000000  15120.000000  15120.000000  15120.000000  15120.000000   \n",
       "mean       0.045635      0.040741      0.001455      0.006746      0.000661   \n",
       "std        0.208699      0.197696      0.038118      0.081859      0.025710   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "        Soil_Type37   Soil_Type38   Soil_Type39   Soil_Type40    Cover_Type  \n",
       "count  15120.000000  15120.000000  15120.000000  15120.000000  15120.000000  \n",
       "mean       0.002249      0.048148      0.043452      0.030357      4.000000  \n",
       "std        0.047368      0.214086      0.203880      0.171574      2.000066  \n",
       "min        0.000000      0.000000      0.000000      0.000000      1.000000  \n",
       "25%        0.000000      0.000000      0.000000      0.000000      2.000000  \n",
       "50%        0.000000      0.000000      0.000000      0.000000      4.000000  \n",
       "75%        0.000000      0.000000      0.000000      0.000000      6.000000  \n",
       "max        1.000000      1.000000      1.000000      1.000000      7.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data description - statistical\n",
    "pd.set_option('display.max_columns', None)\n",
    "dataset.describe()\n",
    "# Learning :\n",
    "# No attribute is missing as count is 15120 for all attributes. Hence, all rows can be used\n",
    "# Wilderness_Area and Soil_Type are one hot encoded. Hence, they could be converted back for some analysis\n",
    "# Attributes Soil_Type7 and Soil_Type15 can be removed as they are constant\n",
    "# Scales are not the same for all. Hence, rescaling and standardization may be necessary for some algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cover_Type\n",
       "1    2160\n",
       "2    2160\n",
       "3    2160\n",
       "4    2160\n",
       "5    2160\n",
       "6    2160\n",
       "7    2160\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classification distribution\n",
    "dataset.groupby('Cover_Type').size()\n",
    "\n",
    "#learning：equally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hillshade_9am and Hillshade_3pm co-efficent is -0.78\n",
      "Horizontal_Distance_To_Hydrology and Vertical_Distance_To_Hydrology co-efficent is 0.65\n",
      "Aspect and Hillshade_3pm co-efficent is 0.64\n",
      "Hillshade_Noon and Hillshade_3pm co-efficent is 0.61\n",
      "Slope and Hillshade_Noon co-efficent is -0.61\n",
      "Aspect and Hillshade_9am co-efficent is -0.59\n",
      "Elevation and Horizontal_Distance_To_Roadways co-efficent is 0.58\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "# Correlation requires continous data. Hence, ignore Wilderness_Area and Soil_Type as they are binary\n",
    "size = 10\n",
    "data_corr = dataset.iloc[:,:size]\n",
    "\n",
    "cols = data_corr.columns\n",
    "\n",
    "# Calculates pearson co-efficient for all combinations\n",
    "data_corr_coeff = data_corr.corr()\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "high_corr_list = []\n",
    "\n",
    "for i in range(0,size):\n",
    "    for j in range(i+1, size):\n",
    "        if(data_corr_coeff.iloc[i,j]>=threshold or data_corr_coeff.iloc[i,j]<=-threshold):\n",
    "            high_corr_list.append([data_corr_coeff.iloc[i,j],i,j])\n",
    "s_high_corr_list = sorted(high_corr_list,key=lambda x: -abs(x[0]))\n",
    "for v, i, j in s_high_corr_list:\n",
    "    print (\"%s and %s co-efficent is %.2f\" % (cols[i],cols[j],v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Soil_Type7', 'Soil_Type15']\n"
     ]
    }
   ],
   "source": [
    "#remove the constant columns\n",
    "\n",
    "const = []\n",
    "\n",
    "for col in dataset.columns:\n",
    "    if dataset[col].std()==0:\n",
    "        const.append(col)\n",
    "\n",
    "dataset.drop(const,axis=1,inplace=True)\n",
    "\n",
    "print(const)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/binluo/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/binluo/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/binluo/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/binluo/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/binluo/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/binluo/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/binluo/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/binluo/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#obtain new # of row and column\n",
    "r, c = dataset.shape\n",
    "\n",
    "#get columns' name\n",
    "cols = dataset.columns\n",
    "\n",
    "array = dataset.values\n",
    "\n",
    "X = array[:,0:(c-1)]\n",
    "Y = array[:,(c-1)]\n",
    "\n",
    "val_size = 0.1\n",
    "\n",
    "seed = 42\n",
    "\n",
    "#Split the data into chunks\n",
    "from sklearn import cross_validation\n",
    "X_train, X_val, Y_train, Y_val = cross_validation.train_test_split(X, Y, test_size=val_size,random_state=seed)\n",
    "\n",
    "#Import libraries for data transformations\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "X_all = []\n",
    "X_all.append(['Orig','All',X_train,X_val])\n",
    "\n",
    "#size of the non-categorical data\n",
    "size = 10\n",
    "\n",
    "#standardized\n",
    "X_temp = StandardScaler().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = StandardScaler().fit_transform(X_val[:,0:size])\n",
    "#Concatenate non-categorical data and categorical\n",
    "X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "#Add this version of X to the list \n",
    "X_all.append(['StdSca','All', X_con,X_val_con])\n",
    "\n",
    "#MinMax\n",
    "#Apply transform only for non-categorical data\n",
    "X_temp = MinMaxScaler().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = MinMaxScaler().fit_transform(X_val[:,0:size])\n",
    "#Concatenate non-categorical data and categorical\n",
    "X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "#Add this version of X to the list \n",
    "X_all.append(['MinMax', 'All', X_con,X_val_con])\n",
    "\n",
    "#Normalize\n",
    "#Apply transform only for non-categorical data\n",
    "X_temp = Normalizer().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = Normalizer().fit_transform(X_val[:,0:size])\n",
    "#Concatenate non-categorical data and categorical\n",
    "X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "#Add this version of X to the list \n",
    "X_all.append(['Norm', 'All', X_con,X_val_con])\n",
    "\n",
    "#List of transformations\n",
    "transform_list = []\n",
    "\n",
    "for trans,name,X,X_val in X_all:\n",
    "    transform_list.append(trans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection - based on feature importance\n",
    "* ExtraTreesClassifier\n",
    "* RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select top 75%\n",
    "ratios = [0.75,0.5]\n",
    "\n",
    "#Import the libraries\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = []\n",
    "models.append(['Extra',ExtraTreesClassifier(n_estimators=c-1,n_jobs=-1,random_state=seed)])\n",
    "models.append(['RndFst',RandomForestClassifier(n_estimators=c-1,n_jobs=-1,random_state=seed)])\n",
    "\n",
    "X_select = []\n",
    "\n",
    "#for all transformation of X_all\n",
    "for trans, p, X, X_val in X_all:\n",
    "    #for all feature importance based selection models\n",
    "    for name, model in models:\n",
    "        #train model to get feature importance\n",
    "        model.fit(X,Y_train)\n",
    "        # sort & store feature importance\n",
    "        feat_imp = []\n",
    "        for i, imp in enumerate(list(model.feature_importances_)):\n",
    "            feat_imp.append([i,cols[i],imp])\n",
    "        \n",
    "        imp_sorted = sorted(feat_imp,key=lambda x: -x[2])\n",
    "        \n",
    "        #loop through different ratio\n",
    "        for ratio in ratios:\n",
    "            #store selected columns based on ratio\n",
    "            remove_start = int((ratio*(c-1)))\n",
    "            #store ranking for each feature\n",
    "            ranking = []\n",
    "            select_list = []\n",
    "            remove_list = []\n",
    "            for j, (i,col,x) in enumerate(list(imp_sorted)):\n",
    "                ranking.append([i,cols[i],j])\n",
    "                if (j<remove_start):\n",
    "                    select_list.append([i,col,j])\n",
    "                else:\n",
    "                    remove_list.append([i,col,j])\n",
    "            ranking_sorted = sorted(ranking,key=lambda x: x[0])\n",
    "            X_select.append([trans,name,ratio,[x[2] for x in ranking_sorted],select_list,remove_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection - RFE, based on feature ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Libraries for feature selection\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LG_model = LogisticRegression(random_state=seed,n_jobs=-1)\n",
    "RFE_models=[]\n",
    "RFE_models.append(['RFE_0.75',RFE(LG_model,0.75*(c-1)),0.75])\n",
    "RFE_models.append(['RFE_0.5',RFE(LG_model,0.5*(c-1)),0.5])\n",
    "\n",
    "#for all transformation of X_all\n",
    "for trans, p, X, X_val in X_all:\n",
    "    #for all feature importance based selection models\n",
    "    for name, model,ratio in RFE_models:\n",
    "        #train model to get feature importance\n",
    "        model.fit(X,Y_train)\n",
    "        # sort & store feature importance\n",
    "        feat_imp = []\n",
    "        for i, imp in enumerate(list(model.ranking_)):\n",
    "            feat_imp.append([i,cols[i],imp])\n",
    "        \n",
    "        imp_sorted = sorted(feat_imp,key=lambda x: -x[2])\n",
    "        \n",
    "        #remove starting point\n",
    "        remove_start = int((ratio*(c-1)))\n",
    "        select_list = []\n",
    "        remove_list = []\n",
    "        for j, (i,col,x) in enumerate(list(imp_sorted)):\n",
    "\n",
    "            if (j<remove_start):\n",
    "                select_list.append([i,col,j])\n",
    "            else:\n",
    "                remove_list.append([i,col,j])\n",
    "\n",
    "        X_select.append([trans,name,ratio,[x[2] for x in feat_imp],select_list,remove_list])\n",
    "#print X_select    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import plotting library    \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "rank_summary = pd.DataFrame(data=[x[3] for x in X_select],columns=cols[:c-1])\n",
    "rank_median = rank_summary.median()\n",
    "_ = rank_summary.boxplot(rot=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction, Evaluation & Analysis\n",
    "* LDA - linear algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig + All, 0.661376\n",
      "StdSca + All, 0.659392\n",
      "MinMax + All, 0.625000\n",
      "Norm + All, 0.651455\n",
      "Orig + Extra, #of used features is 39, accuracy is 0.657407\n",
      "Orig + Extra, #of used features is 26, accuracy is 0.638228\n",
      "Orig + RndFst, #of used features is 39, accuracy is 0.659392\n",
      "Orig + RndFst, #of used features is 26, accuracy is 0.646825\n",
      "StdSca + Extra, #of used features is 39, accuracy is 0.658069\n",
      "StdSca + Extra, #of used features is 26, accuracy is 0.638889\n",
      "StdSca + RndFst, #of used features is 39, accuracy is 0.658730\n",
      "StdSca + RndFst, #of used features is 26, accuracy is 0.646825\n",
      "MinMax + Extra, #of used features is 39, accuracy is 0.619709\n",
      "MinMax + Extra, #of used features is 26, accuracy is 0.606481\n",
      "MinMax + RndFst, #of used features is 39, accuracy is 0.621032\n",
      "MinMax + RndFst, #of used features is 26, accuracy is 0.607804\n",
      "Norm + Extra, #of used features is 39, accuracy is 0.648148\n",
      "Norm + Extra, #of used features is 26, accuracy is 0.628968\n",
      "Norm + RndFst, #of used features is 39, accuracy is 0.649471\n",
      "Norm + RndFst, #of used features is 26, accuracy is 0.628968\n",
      "Orig + RFE_0.75, #of used features is 39, accuracy is 0.655423\n",
      "Orig + RFE_0.5, #of used features is 26, accuracy is 0.631614\n",
      "StdSca + RFE_0.75, #of used features is 39, accuracy is 0.648148\n",
      "StdSca + RFE_0.5, #of used features is 26, accuracy is 0.431878\n",
      "MinMax + RFE_0.75, #of used features is 39, accuracy is 0.581349\n",
      "MinMax + RFE_0.5, #of used features is 26, accuracy is 0.347884\n",
      "Norm + RFE_0.75, #of used features is 39, accuracy is 0.584656\n",
      "Norm + RFE_0.5, #of used features is 26, accuracy is 0.441799\n",
      "['LDA_Orig_All_1.00', 0.66137566137566139]\n"
     ]
    }
   ],
   "source": [
    "#create a list to store all the all top-performance combination across all the tested algorithms \n",
    "best_algo = []\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "model = LinearDiscriminantAnalysis()\n",
    "#model = LogisticRegression(n_jobs=-1,random_state=seed,C=100)\n",
    "algo_name = 'LDA'\n",
    "\n",
    "#placeholder to store the combination with best performance for this algorithm\n",
    "best_result = ['placeholder',0.0]\n",
    "\n",
    "#get the performances of using all features and different transformed data to train the model\n",
    "for trans, p, x_t, x_v in X_all:\n",
    "    model.fit(x_t,Y_train)\n",
    "    result = model.score(x_v,Y_val)\n",
    "    if (result > best_result[1]):\n",
    "        best_result[0] = algo_name+'_'+trans+'_'+p+'_1.00'\n",
    "        best_result[1] = result\n",
    "    print(trans+\" + \"+p+\", %f\") % result\n",
    "    \n",
    "#performance of using subset of the features and different transformed data to train the model\n",
    "for trans, name, r, rank, select_list,remove_list in X_select:\n",
    "    #get the indexes of the features selected \n",
    "    i_cols_list = [x[0] for x in select_list]\n",
    "    #get the tranformed data based for different\n",
    "    for t, n, x_t, x_v in X_all:\n",
    "        if (trans==t):\n",
    "            model.fit(x_t[:,i_cols_list],Y_train)\n",
    "            result = model.score(x_v[:,i_cols_list], Y_val)\n",
    "            if (result > best_result[1]):\n",
    "                best_result[0] = algo_name+'_'+trans+'_'+name+'_'+r\n",
    "                best_result[1] = result\n",
    "    print(trans+\" + \"+name+\", #of used features is %d, accuracy is %f\" % (r*(c-1),result))\n",
    "best_algo.append(best_result)\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction, Evaluation & Analysis\n",
    "* LR - logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig + All, 0.679233\n",
      "StdSca + All, 0.693783\n",
      "MinMax + All, 0.639550\n",
      "Norm + All, 0.668651\n",
      "Orig + Extra, #of used features is 39, accuracy is 0.672619\n",
      "Orig + Extra, #of used features is 26, accuracy is 0.660053\n",
      "Orig + RndFst, #of used features is 39, accuracy is 0.663360\n",
      "Orig + RndFst, #of used features is 26, accuracy is 0.654762\n",
      "StdSca + Extra, #of used features is 39, accuracy is 0.686508\n",
      "StdSca + Extra, #of used features is 26, accuracy is 0.658069\n",
      "StdSca + RndFst, #of used features is 39, accuracy is 0.685185\n",
      "StdSca + RndFst, #of used features is 26, accuracy is 0.658730\n",
      "MinMax + Extra, #of used features is 39, accuracy is 0.640873\n",
      "MinMax + Extra, #of used features is 26, accuracy is 0.631614\n",
      "MinMax + RndFst, #of used features is 39, accuracy is 0.640873\n",
      "MinMax + RndFst, #of used features is 26, accuracy is 0.636243\n",
      "Norm + Extra, #of used features is 39, accuracy is 0.660714\n",
      "Norm + Extra, #of used features is 26, accuracy is 0.635582\n",
      "Norm + RndFst, #of used features is 39, accuracy is 0.657407\n",
      "Norm + RndFst, #of used features is 26, accuracy is 0.635582\n",
      "Orig + RFE_0.75, #of used features is 39, accuracy is 0.654101\n",
      "Orig + RFE_0.5, #of used features is 26, accuracy is 0.594577\n",
      "StdSca + RFE_0.75, #of used features is 39, accuracy is 0.665344\n",
      "StdSca + RFE_0.5, #of used features is 26, accuracy is 0.443122\n",
      "MinMax + RFE_0.75, #of used features is 39, accuracy is 0.593254\n",
      "MinMax + RFE_0.5, #of used features is 26, accuracy is 0.374339\n",
      "Norm + RFE_0.75, #of used features is 39, accuracy is 0.616402\n",
      "Norm + RFE_0.5, #of used features is 26, accuracy is 0.439815\n",
      "['LR_StdSca_All_1.00', 0.69378306878306883]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(n_jobs=-1,random_state=seed,C=100)\n",
    "algo_name = 'LR'\n",
    "\n",
    "best_result = ['placeholder',0.0]\n",
    "#performances of using all features and different transformed data to train the model\n",
    "for trans, p, x_t, x_v in X_all:\n",
    "    model.fit(x_t,Y_train)\n",
    "    result = model.score(x_v,Y_val)\n",
    "    if (result > best_result[1]):\n",
    "        best_result[0] = algo_name+'_'+trans+'_'+p+'_1.00'\n",
    "        best_result[1] = result\n",
    "    print(trans+\" + \"+p+\", %f\") % result\n",
    "    \n",
    "#performance of using subset of the features and different transformed data to train the model\n",
    "for trans, name, r, rank, select_list,remove_list in X_select:\n",
    "    #get the indexes of the features selected \n",
    "    i_cols_list = [x[0] for x in select_list]\n",
    "    #get the tranformed data based for different\n",
    "    for t, n, x_t, x_v in X_all:\n",
    "        if (trans==t):\n",
    "            model.fit(x_t[:,i_cols_list],Y_train)\n",
    "            result = model.score(x_v[:,i_cols_list], Y_val)\n",
    "            if (result > best_result[1]):\n",
    "                best_result[0] = algo_name+'_'+trans+'_'+name+'_'+r\n",
    "                best_result[1] = result\n",
    "    print(trans+\" + \"+name+\", #of used features is %d, accuracy is %f\" % (r*(c-1),result))\n",
    "best_algo.append(best_result)\n",
    "print(best_result)\n",
    "\n",
    "#training time is longer than LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction, Evaluation & Analysis\n",
    "* KNN - Non linear, K-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig + All, 0.857804\n",
      "StdSca + All, 0.822090\n",
      "MinMax + All, 0.812169\n",
      "Norm + All, 0.873677\n",
      "Orig + Extra, #of used features is 39, accuracy is 0.857804\n",
      "Orig + Extra, #of used features is 26, accuracy is 0.857804\n",
      "Orig + RndFst, #of used features is 39, accuracy is 0.857804\n",
      "Orig + RndFst, #of used features is 26, accuracy is 0.857804\n",
      "StdSca + Extra, #of used features is 39, accuracy is 0.822090\n",
      "StdSca + Extra, #of used features is 26, accuracy is 0.816138\n",
      "StdSca + RndFst, #of used features is 39, accuracy is 0.822090\n",
      "StdSca + RndFst, #of used features is 26, accuracy is 0.818122\n",
      "MinMax + Extra, #of used features is 39, accuracy is 0.812169\n",
      "MinMax + Extra, #of used features is 26, accuracy is 0.798280\n",
      "MinMax + RndFst, #of used features is 39, accuracy is 0.812169\n",
      "MinMax + RndFst, #of used features is 26, accuracy is 0.802249\n",
      "Norm + Extra, #of used features is 39, accuracy is 0.869709\n",
      "Norm + Extra, #of used features is 26, accuracy is 0.858466\n",
      "Norm + RndFst, #of used features is 39, accuracy is 0.870370\n",
      "Norm + RndFst, #of used features is 26, accuracy is 0.858466\n",
      "Orig + RFE_0.75, #of used features is 39, accuracy is 0.857804\n",
      "Orig + RFE_0.5, #of used features is 26, accuracy is 0.857804\n",
      "StdSca + RFE_0.75, #of used features is 39, accuracy is 0.808862\n",
      "StdSca + RFE_0.5, #of used features is 26, accuracy is 0.715608\n",
      "MinMax + RFE_0.75, #of used features is 39, accuracy is 0.792328\n",
      "MinMax + RFE_0.5, #of used features is 26, accuracy is 0.315476\n",
      "Norm + RFE_0.75, #of used features is 39, accuracy is 0.859788\n",
      "Norm + RFE_0.5, #of used features is 26, accuracy is 0.617725\n",
      "['KNN_Norm_All_1.00', 0.87367724867724872]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#creat model with 1 neighbor\n",
    "model = KNeighborsClassifier(n_jobs=-1,n_neighbors=1)\n",
    "algo_name = 'KNN'\n",
    "\n",
    "best_result = ['placeholder',0.0]\n",
    "#performances of using all features\n",
    "for trans, p, x_t, x_v in X_all:\n",
    "    model.fit(x_t,Y_train)\n",
    "    result = model.score(x_v,Y_val)\n",
    "    if (result > best_result[1]):\n",
    "        best_result[0] = algo_name+'_'+trans+'_'+p+'_1.00'\n",
    "        best_result[1] = result\n",
    "    print(trans+\" + \"+p+\", %f\") % result\n",
    "    \n",
    "#performance of using subset of the features \n",
    "for trans, name, r, rank, select_list,remove_list in X_select:\n",
    "    #get the indexes of the features selected \n",
    "    i_cols_list = [x[0] for x in select_list]\n",
    "    #get the tranformed data based for different\n",
    "    for t, n, x_t, x_v in X_all:\n",
    "        if (trans==t):\n",
    "            model.fit(x_t[:,i_cols_list],Y_train)\n",
    "            result = model.score(x_v[:,i_cols_list], Y_val)\n",
    "            if (result > best_result[1]):\n",
    "                best_result[0] = algo_name+'_'+trans+'_'+name+'_'+r\n",
    "                best_result[1] = result\n",
    "    print(trans+\" + \"+name+\", #of used features is %d, accuracy is %f\" % (r*(c-1),result))\n",
    "best_algo.append(best_result)\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction, Evaluation & Analysis\n",
    "* CART - Non-linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig + All, 0.784392\n",
      "StdSca + All, 0.775794\n",
      "MinMax + All, 0.699074\n",
      "Norm + All, 0.710979\n",
      "Orig + Extra, #of used features is 39, accuracy is 0.782407\n",
      "Orig + Extra, #of used features is 26, accuracy is 0.781746\n",
      "Orig + RndFst, #of used features is 39, accuracy is 0.784392\n",
      "Orig + RndFst, #of used features is 26, accuracy is 0.777778\n",
      "StdSca + Extra, #of used features is 39, accuracy is 0.773810\n",
      "StdSca + Extra, #of used features is 26, accuracy is 0.771164\n",
      "StdSca + RndFst, #of used features is 39, accuracy is 0.773810\n",
      "StdSca + RndFst, #of used features is 26, accuracy is 0.775132\n",
      "MinMax + Extra, #of used features is 39, accuracy is 0.693783\n",
      "MinMax + Extra, #of used features is 26, accuracy is 0.667989\n",
      "MinMax + RndFst, #of used features is 39, accuracy is 0.687169\n",
      "MinMax + RndFst, #of used features is 26, accuracy is 0.674603\n",
      "Norm + Extra, #of used features is 39, accuracy is 0.709656\n",
      "Norm + Extra, #of used features is 26, accuracy is 0.706349\n",
      "Norm + RndFst, #of used features is 39, accuracy is 0.710317\n",
      "Norm + RndFst, #of used features is 26, accuracy is 0.705026\n",
      "Orig + RFE_0.75, #of used features is 39, accuracy is 0.773810\n",
      "Orig + RFE_0.5, #of used features is 26, accuracy is 0.742063\n",
      "StdSca + RFE_0.75, #of used features is 39, accuracy is 0.771825\n",
      "StdSca + RFE_0.5, #of used features is 26, accuracy is 0.597884\n",
      "MinMax + RFE_0.75, #of used features is 39, accuracy is 0.667989\n",
      "MinMax + RFE_0.5, #of used features is 26, accuracy is 0.390212\n",
      "Norm + RFE_0.75, #of used features is 39, accuracy is 0.744048\n",
      "Norm + RFE_0.5, #of used features is 26, accuracy is 0.638228\n",
      "['CART_Orig_All_1.00', 0.78439153439153442]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=seed,max_depth=13)\n",
    "algo_name = 'CART'\n",
    "\n",
    "best_result = ['placeholder',0.0]\n",
    "#performances of using all features\n",
    "for trans, p, x_t, x_v in X_all:\n",
    "    model.fit(x_t,Y_train)\n",
    "    result = model.score(x_v,Y_val)\n",
    "    if (result > best_result[1]):\n",
    "        best_result[0] = algo_name+'_'+trans+'_'+p+'_1.00'\n",
    "        best_result[1] = result\n",
    "    print(trans+\" + \"+p+\", %f\") % result\n",
    "    \n",
    "#performance of using subset of the features \n",
    "for trans, name, r, rank, select_list,remove_list in X_select:\n",
    "    #get the indexes of the features selected \n",
    "    i_cols_list = [x[0] for x in select_list]\n",
    "    #get the tranformed data based for different\n",
    "    for t, n, x_t, x_v in X_all:\n",
    "        if (trans==t):\n",
    "            model.fit(x_t[:,i_cols_list],Y_train)\n",
    "            result = model.score(x_v[:,i_cols_list], Y_val)\n",
    "            if (result > best_result[1]):\n",
    "                best_result[0] = algo_name+'_'+trans+'_'+name+'_'+ str(r)\n",
    "                best_result[1] = result\n",
    "    print(trans+\" + \"+name+\", #of used features is %d, accuracy is %f\" % (r*(c-1),result))\n",
    "best_algo.append(best_result)\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction, Evaluation & Analysis\n",
    "* SVM - Non-linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig + All, 0.140212\n",
      "StdSca + All, 0.792328\n",
      "MinMax + All, 0.700397\n",
      "Norm + All, 0.629630\n",
      "Orig + Extra, #of used features is 39, accuracy is 0.137566\n",
      "Orig + Extra, #of used features is 26, accuracy is 0.136243\n",
      "Orig + RndFst, #of used features is 39, accuracy is 0.137566\n",
      "Orig + RndFst, #of used features is 26, accuracy is 0.136243\n",
      "StdSca + Extra, #of used features is 39, accuracy is 0.794974\n",
      "StdSca + Extra, #of used features is 26, accuracy is 0.792989\n",
      "StdSca + RndFst, #of used features is 39, accuracy is 0.794312\n",
      "StdSca + RndFst, #of used features is 26, accuracy is 0.794974\n",
      "MinMax + Extra, #of used features is 39, accuracy is 0.703704\n",
      "MinMax + Extra, #of used features is 26, accuracy is 0.697751\n",
      "MinMax + RndFst, #of used features is 39, accuracy is 0.705026\n",
      "MinMax + RndFst, #of used features is 26, accuracy is 0.694444\n",
      "Norm + Extra, #of used features is 39, accuracy is 0.626323\n",
      "Norm + Extra, #of used features is 26, accuracy is 0.619048\n",
      "Norm + RndFst, #of used features is 39, accuracy is 0.626323\n",
      "Norm + RndFst, #of used features is 26, accuracy is 0.619048\n",
      "Orig + RFE_0.75, #of used features is 39, accuracy is 0.137566\n",
      "Orig + RFE_0.5, #of used features is 26, accuracy is 0.136243\n",
      "StdSca + RFE_0.75, #of used features is 39, accuracy is 0.781746\n",
      "StdSca + RFE_0.5, #of used features is 26, accuracy is 0.622354\n",
      "MinMax + RFE_0.75, #of used features is 39, accuracy is 0.689815\n",
      "MinMax + RFE_0.5, #of used features is 26, accuracy is 0.369709\n",
      "Norm + RFE_0.75, #of used features is 39, accuracy is 0.564815\n",
      "Norm + RFE_0.5, #of used features is 26, accuracy is 0.477513\n",
      "['CART_StdSca_Extra_0.75', 0.794973544973545]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(random_state=seed,C=10)\n",
    "\n",
    "best_result = ['placeholder',0.0]\n",
    "#performances of using all features\n",
    "for trans, p, x_t, x_v in X_all:\n",
    "    model.fit(x_t,Y_train)\n",
    "    result = model.score(x_v,Y_val)\n",
    "    if (result > best_result[1]):\n",
    "        best_result[0] = algo_name+'_'+trans+'_'+p+'_1.00'\n",
    "        best_result[1] = result\n",
    "    print(trans+\" + \"+p+\", %f\") % result\n",
    "    \n",
    "#performance of using subset of the features \n",
    "for trans, name, r, rank, select_list,remove_list in X_select:\n",
    "    #get the indexes of the features selected \n",
    "    i_cols_list = [x[0] for x in select_list]\n",
    "    #get the tranformed data based for different\n",
    "    for t, n, x_t, x_v in X_all:\n",
    "        if (trans==t):\n",
    "            model.fit(x_t[:,i_cols_list],Y_train)\n",
    "            result = model.score(x_v[:,i_cols_list], Y_val)\n",
    "            if (result > best_result[1]):\n",
    "                best_result[0] = algo_name+'_'+trans+'_'+name+'_'+str(r)\n",
    "                best_result[1] = result\n",
    "    print(trans+\" + \"+name+\", #of used features is %d, accuracy is %f\" % (r*(c-1),result))\n",
    "best_algo.append(best_result)\n",
    "print(best_result)\n",
    "\n",
    "#Training time is very long compared to other algorithms\n",
    "#performance is very poor for Original data, data preprocessing is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction, Evaluation & Analysis\n",
    "* RandomForest - bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig + All, 0.876984\n",
      "StdSca + All, 0.871032\n",
      "MinMax + All, 0.833995\n",
      "Norm + All, 0.869048\n",
      "Orig + Extra, #of used features is 39, accuracy is 0.880291\n",
      "Orig + Extra, #of used features is 26, accuracy is 0.867725\n",
      "Orig + RndFst, #of used features is 39, accuracy is 0.874339\n",
      "Orig + RndFst, #of used features is 26, accuracy is 0.871693\n",
      "StdSca + Extra, #of used features is 39, accuracy is 0.880952\n",
      "StdSca + Extra, #of used features is 26, accuracy is 0.868386\n",
      "StdSca + RndFst, #of used features is 39, accuracy is 0.879630\n",
      "StdSca + RndFst, #of used features is 26, accuracy is 0.878307\n",
      "MinMax + Extra, #of used features is 39, accuracy is 0.822751\n",
      "MinMax + Extra, #of used features is 26, accuracy is 0.808862\n",
      "MinMax + RndFst, #of used features is 39, accuracy is 0.831349\n",
      "MinMax + RndFst, #of used features is 26, accuracy is 0.807540\n",
      "Norm + Extra, #of used features is 39, accuracy is 0.869709\n",
      "Norm + Extra, #of used features is 26, accuracy is 0.869709\n",
      "Norm + RndFst, #of used features is 39, accuracy is 0.867725\n",
      "Norm + RndFst, #of used features is 26, accuracy is 0.867725\n",
      "Orig + RFE_0.75, #of used features is 39, accuracy is 0.878968\n",
      "Orig + RFE_0.5, #of used features is 26, accuracy is 0.849206\n",
      "StdSca + RFE_0.75, #of used features is 39, accuracy is 0.871693\n",
      "StdSca + RFE_0.5, #of used features is 26, accuracy is 0.761905\n",
      "MinMax + RFE_0.75, #of used features is 39, accuracy is 0.812169\n",
      "MinMax + RFE_0.5, #of used features is 26, accuracy is 0.355820\n",
      "Norm + RFE_0.75, #of used features is 39, accuracy is 0.868386\n",
      "Norm + RFE_0.5, #of used features is 26, accuracy is 0.697751\n",
      "['RndFst_StdSca_Extra_0.75', 0.88095238095238093]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=-1,n_estimators=100, random_state=seed)\n",
    "algo_name = \"RndFst\"\n",
    "\n",
    "best_result = ['placeholder',0.0]\n",
    "#performances of using all features\n",
    "for trans, p, x_t, x_v in X_all:\n",
    "    model.fit(x_t,Y_train)\n",
    "    result = model.score(x_v,Y_val)\n",
    "    if (result > best_result[1]):\n",
    "        best_result[0] = algo_name+'_'+trans+'_'+p+'_1.00'\n",
    "        best_result[1] = result\n",
    "    print(trans+\" + \"+p+\", %f\") % result\n",
    "    \n",
    "#performance of using subset of the features \n",
    "for trans, name, r, rank, select_list,remove_list in X_select:\n",
    "    #get the indexes of the features selected \n",
    "    i_cols_list = [x[0] for x in select_list]\n",
    "    #get the tranformed data based for different\n",
    "    for t, n, x_t, x_v in X_all:\n",
    "        if (trans==t):\n",
    "            model.fit(x_t[:,i_cols_list],Y_train)\n",
    "            result = model.score(x_v[:,i_cols_list], Y_val)\n",
    "            if (result > best_result[1]):\n",
    "                best_result[0] = algo_name+'_'+trans+'_'+name+'_'+str(r)\n",
    "                best_result[1] = result\n",
    "    print(trans+\" + \"+name+\", #of used features is %d, accuracy is %f\" % (r*(c-1),result))\n",
    "best_algo.append(best_result)\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction, Evaluation & Analysis\n",
    "* Extra Trees - bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig + All, 0.882275\n",
      "StdSca + All, 0.885582\n",
      "MinMax + All, 0.849206\n",
      "Norm + All, 0.873677\n",
      "Orig + Extra, #of used features is 39, accuracy is 0.879630\n",
      "Orig + Extra, #of used features is 26, accuracy is 0.876323\n",
      "Orig + RndFst, #of used features is 39, accuracy is 0.875661\n",
      "Orig + RndFst, #of used features is 26, accuracy is 0.880291\n",
      "StdSca + Extra, #of used features is 39, accuracy is 0.876984\n",
      "StdSca + Extra, #of used features is 26, accuracy is 0.877646\n",
      "StdSca + RndFst, #of used features is 39, accuracy is 0.884921\n",
      "StdSca + RndFst, #of used features is 26, accuracy is 0.875661\n",
      "MinMax + Extra, #of used features is 39, accuracy is 0.843254\n",
      "MinMax + Extra, #of used features is 26, accuracy is 0.824074\n",
      "MinMax + RndFst, #of used features is 39, accuracy is 0.845899\n",
      "MinMax + RndFst, #of used features is 26, accuracy is 0.827381\n",
      "Norm + Extra, #of used features is 39, accuracy is 0.879630\n",
      "Norm + Extra, #of used features is 26, accuracy is 0.872354\n",
      "Norm + RndFst, #of used features is 39, accuracy is 0.878307\n",
      "Norm + RndFst, #of used features is 26, accuracy is 0.870370\n",
      "Orig + RFE_0.75, #of used features is 39, accuracy is 0.884921\n",
      "Orig + RFE_0.5, #of used features is 26, accuracy is 0.853836\n",
      "StdSca + RFE_0.75, #of used features is 39, accuracy is 0.877646\n",
      "StdSca + RFE_0.5, #of used features is 26, accuracy is 0.759259\n",
      "MinMax + RFE_0.75, #of used features is 39, accuracy is 0.830688\n",
      "MinMax + RFE_0.5, #of used features is 26, accuracy is 0.363757\n",
      "Norm + RFE_0.75, #of used features is 39, accuracy is 0.873677\n",
      "Norm + RFE_0.5, #of used features is 26, accuracy is 0.696429\n",
      "['ET_StdSca_All_1.00', 0.88558201058201058]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "model = ExtraTreesClassifier(n_jobs=-1,n_estimators=100, random_state=seed)\n",
    "algo_name = 'ET'\n",
    "\n",
    "best_result = ['placeholder',0.0]\n",
    "#performances of using all features\n",
    "for trans, p, x_t, x_v in X_all:\n",
    "    model.fit(x_t,Y_train)\n",
    "    result = model.score(x_v,Y_val)\n",
    "    if (result > best_result[1]):\n",
    "        best_result[0] = algo_name+'_'+trans+'_'+p+'_1.00'\n",
    "        best_result[1] = result\n",
    "    print(trans+\" + \"+p+\", %f\") % result\n",
    "    \n",
    "#performance of using subset of the features \n",
    "for trans, name, r, rank, select_list,remove_list in X_select:\n",
    "    #get the indexes of the features selected \n",
    "    i_cols_list = [x[0] for x in select_list]\n",
    "    #get the tranformed data based for different\n",
    "    for t, n, x_t, x_v in X_all:\n",
    "        if (trans==t):\n",
    "            model.fit(x_t[:,i_cols_list],Y_train)\n",
    "            result = model.score(x_v[:,i_cols_list], Y_val)\n",
    "            if (result > best_result[1]):\n",
    "                best_result[0] = algo_name+'_'+trans+'_'+name+'_'+str(r)\n",
    "                best_result[1] = result\n",
    "    print(trans+\" + \"+name+\", #of used features is %d, accuracy is %f\" % (r*(c-1),result))\n",
    "best_algo.append(best_result)\n",
    "print(best_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction, Evaluation & Analysis\n",
    "* AdaBoost - Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig + All, 0.388889\n",
      "StdSca + All, 0.388228\n",
      "MinMax + All, 0.385582\n",
      "Norm + All, 0.362434\n",
      "Orig + Extra, #of used features is 39, accuracy is 0.388889\n",
      "Orig + Extra, #of used features is 26, accuracy is 0.388889\n",
      "Orig + RndFst, #of used features is 39, accuracy is 0.388889\n",
      "Orig + RndFst, #of used features is 26, accuracy is 0.388889\n",
      "StdSca + Extra, #of used features is 39, accuracy is 0.388228\n",
      "StdSca + Extra, #of used features is 26, accuracy is 0.388228\n",
      "StdSca + RndFst, #of used features is 39, accuracy is 0.388228\n",
      "StdSca + RndFst, #of used features is 26, accuracy is 0.388228\n",
      "MinMax + Extra, #of used features is 39, accuracy is 0.385582\n",
      "MinMax + Extra, #of used features is 26, accuracy is 0.385582\n",
      "MinMax + RndFst, #of used features is 39, accuracy is 0.385582\n",
      "MinMax + RndFst, #of used features is 26, accuracy is 0.385582\n",
      "Norm + Extra, #of used features is 39, accuracy is 0.420635\n",
      "Norm + Extra, #of used features is 26, accuracy is 0.377646\n",
      "Norm + RndFst, #of used features is 39, accuracy is 0.420635\n",
      "Norm + RndFst, #of used features is 26, accuracy is 0.377646\n",
      "Orig + RFE_0.75, #of used features is 39, accuracy is 0.388889\n",
      "Orig + RFE_0.5, #of used features is 26, accuracy is 0.392196\n",
      "StdSca + RFE_0.75, #of used features is 39, accuracy is 0.388228\n",
      "StdSca + RFE_0.5, #of used features is 26, accuracy is 0.356481\n",
      "MinMax + RFE_0.75, #of used features is 39, accuracy is 0.385582\n",
      "MinMax + RFE_0.5, #of used features is 26, accuracy is 0.386905\n",
      "Norm + RFE_0.75, #of used features is 39, accuracy is 0.390873\n",
      "Norm + RFE_0.5, #of used features is 26, accuracy is 0.285053\n",
      "['AdaBoost_Norm_Extra_0.75', 0.42063492063492064]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=100, random_state=seed)\n",
    "algo_name = \"AdaBoost\"\n",
    "\n",
    "best_result = ['placeholder',0.0]\n",
    "#performances of using all features\n",
    "for trans, p, x_t, x_v in X_all:\n",
    "    model.fit(x_t,Y_train)\n",
    "    result = model.score(x_v,Y_val)\n",
    "    if (result > best_result[1]):\n",
    "        best_result[0] = algo_name+'_'+trans+'_'+p+'_1.00'\n",
    "        best_result[1] = result\n",
    "    print(trans+\" + \"+p+\", %f\") % result\n",
    "    \n",
    "#performance of using subset of the features \n",
    "for trans, name, r, rank, select_list,remove_list in X_select:\n",
    "    #get the indexes of the features selected \n",
    "    i_cols_list = [x[0] for x in select_list]\n",
    "    #get the tranformed data based for different\n",
    "    for t, n, x_t, x_v in X_all:\n",
    "        if (trans==t):\n",
    "            model.fit(x_t[:,i_cols_list],Y_train)\n",
    "            result = model.score(x_v[:,i_cols_list], Y_val)\n",
    "            if (result > best_result[1]):\n",
    "                best_result[0] = algo_name+'_'+trans+'_'+name+'_'+str(r)\n",
    "                best_result[1] = result\n",
    "    print(trans+\" + \"+name+\", #of used features is %d, accuracy is %f\" % (r*(c-1),result))\n",
    "best_algo.append(best_result)\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction, Evaluation & Analysis\n",
    "* GradientBoostng - Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig + All, 0.882275\n",
      "StdSca + All, 0.876323\n",
      "MinMax + All, 0.788360\n",
      "Norm + All, 0.873677\n",
      "Orig + Extra, #of used features is 39, accuracy is 0.876984\n",
      "Orig + Extra, #of used features is 26, accuracy is 0.868386\n",
      "Orig + RndFst, #of used features is 39, accuracy is 0.883598\n",
      "Orig + RndFst, #of used features is 26, accuracy is 0.861772\n",
      "StdSca + Extra, #of used features is 39, accuracy is 0.871693\n",
      "StdSca + Extra, #of used features is 26, accuracy is 0.870370\n",
      "StdSca + RndFst, #of used features is 39, accuracy is 0.876323\n",
      "StdSca + RndFst, #of used features is 26, accuracy is 0.862434\n",
      "MinMax + Extra, #of used features is 39, accuracy is 0.787698\n",
      "MinMax + Extra, #of used features is 26, accuracy is 0.781746\n",
      "MinMax + RndFst, #of used features is 39, accuracy is 0.785714\n",
      "MinMax + RndFst, #of used features is 26, accuracy is 0.781085\n",
      "Norm + Extra, #of used features is 39, accuracy is 0.875000\n",
      "Norm + Extra, #of used features is 26, accuracy is 0.862434\n",
      "Norm + RndFst, #of used features is 39, accuracy is 0.874339\n",
      "Norm + RndFst, #of used features is 26, accuracy is 0.863095\n",
      "Orig + RFE_0.75, #of used features is 39, accuracy is 0.868386\n",
      "Orig + RFE_0.5, #of used features is 26, accuracy is 0.852513\n",
      "StdSca + RFE_0.75, #of used features is 39, accuracy is 0.865079\n",
      "StdSca + RFE_0.5, #of used features is 26, accuracy is 0.752646\n",
      "MinMax + RFE_0.75, #of used features is 39, accuracy is 0.775132\n",
      "MinMax + RFE_0.5, #of used features is 26, accuracy is 0.376323\n",
      "Norm + RFE_0.75, #of used features is 39, accuracy is 0.871032\n",
      "Norm + RFE_0.5, #of used features is 26, accuracy is 0.658730\n",
      "['GB_Orig_RndFst_0.75', 0.8835978835978836]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(max_depth=9, random_state=seed)\n",
    "algo_name = 'GB'\n",
    "\n",
    "best_result = ['placeholder',0.0]\n",
    "#performances of using all features\n",
    "for trans, p, x_t, x_v in X_all:\n",
    "    model.fit(x_t,Y_train)\n",
    "    result = model.score(x_v,Y_val)\n",
    "    if (result > best_result[1]):\n",
    "        best_result[0] = algo_name+'_'+trans+'_'+p+'_1.00'\n",
    "        best_result[1] = result\n",
    "    print(trans+\" + \"+p+\", %f\") % result\n",
    "    \n",
    "#performance of using subset of the features \n",
    "for trans, name, r, rank, select_list,remove_list in X_select:\n",
    "    #get the indexes of the features selected \n",
    "    i_cols_list = [x[0] for x in select_list]\n",
    "    #get the tranformed data based for different\n",
    "    for t, n, x_t, x_v in X_all:\n",
    "        if (trans==t):\n",
    "            model.fit(x_t[:,i_cols_list],Y_train)\n",
    "            result = model.score(x_v[:,i_cols_list], Y_val)\n",
    "            if (result > best_result[1]):\n",
    "                best_result[0] = algo_name+'_'+trans+'_'+name+'_'+str(r)\n",
    "                best_result[1] = result\n",
    "    print(trans+\" + \"+name+\", #of used features is %d, accuracy is %f\" % (r*(c-1),result))\n",
    "best_algo.append(best_result)\n",
    "print(best_result)\n",
    "\n",
    "#training time is extremely high, though the performance is good, closed to 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Orig_All_1.00', 0.66137566137566139], ['StdSca_All_1.00', 0.69378306878306883], ['Norm_All_1.00', 0.87367724867724872], ['Orig_RndFst_0.75', 0.78637566137566139], ['StdSca_Extra_0.5', 0.794973544973545], ['Orig_Extra_0.75', 0.87896825396825395], ['StdSca_All_1.00', 0.88558201058201058], ['Norm_Extra_0.75', 0.42063492063492064], ['Orig_All_1.00', 0.88227513227513232], ['LDA_Orig_All_1.00', 0.66137566137566139], ['LR_StdSca_All_1.00', 0.69378306878306883], ['KNN_Norm_All_1.00', 0.87367724867724872], ['CART_Orig_RndFst_0.75', 0.78637566137566139], ['CART_StdSca_Extra_0.5', 0.794973544973545], ['RndFst_Orig_Extra_0.75', 0.87896825396825395], ['ET_StdSca_All_1.00', 0.88558201058201058], ['AdaBoost_Norm_Extra_0.75', 0.42063492063492064], ['GB_Orig_All_1.00', 0.88227513227513232]]\n"
     ]
    }
   ],
   "source": [
    "print(best_algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a plot to identify the algorith with highest performance score \n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "objects = [x[0] for x in best_algo]\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [x[1] for x in best_algo]\n",
    " \n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Accuracy on Testing data')\n",
    "plt.title('Model+Feature#+Transformation selection')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Prediction with best model - 1st submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The testing dataset is too big to upload into GitHub, so I compressed it, before you run this command please unzip the test data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_model_pred = ExtraTreesClassifier(n_jobs=-1,n_estimators=100)\n",
    "\n",
    "#read the test dataset\n",
    "dataset_test = pd.read_csv(\"../data_tree/test.csv\")\n",
    "ID = dataset_test['Id']\n",
    "dataset_test.drop(['Id','Soil_Type7', 'Soil_Type15'],axis=1,inplace=True)\n",
    "X_test_pred = dataset_test.values\n",
    "\n",
    "\n",
    "#performances of using all features\n",
    "for trans, p, x_t, x_v in X_all:\n",
    "    if (trans =='StdSca'):\n",
    "        best_model_pred.fit(x_t,Y_train)\n",
    "\n",
    "#standardized\n",
    "X_pred_temp = StandardScaler().fit_transform(X_test_pred[:,0:size])\n",
    "#Concatenate non-categorical data and categorical\n",
    "X_test_pred_con = numpy.concatenate((X_pred_temp,X_test_pred[:,size:]),axis=1)\n",
    "#Add this version of X to the list \n",
    "predictions = best_model_pred.predict(X_test_pred_con)\n",
    "\n",
    "#writing the prediction result for submission\n",
    "with open(\"submission_StdSca.csv\", \"w\") as subfile:\n",
    "    subfile.write(\"Id,Cover_Type\\n\")\n",
    "    for i, pred in enumerate(list(predictions)):\n",
    "        subfile.write(\"%s,%s\\n\"%(ID[i],pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering - 2nd submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 5 2 ..., 3 3 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "best_model_sub = ExtraTreesClassifier(n_jobs=-1,n_estimators=100)\n",
    "\n",
    "#prepare training data\n",
    "dataset = pd.read_csv(\"../data_tree/train.csv\")\n",
    "#ID = dataset['Id']\n",
    "dataset.drop(['Id','Soil_Type7', 'Soil_Type15'],axis=1,inplace=True)\n",
    "\n",
    "#create a functiont to do the feature engineering\n",
    "def feature_engineering(dataset):\n",
    "    dataset_FE = dataset\n",
    "    dataset_FE['Ele_minus_VDtHyd'] = dataset['Elevation'] - dataset['Vertical_Distance_To_Hydrology']\n",
    "    dataset_FE['Ele_plus_VDtHyd'] = dataset['Elevation'] + dataset['Vertical_Distance_To_Hydrology']\n",
    "    dataset_FE['Distance_to_Hydrolody'] = (dataset['Horizontal_Distance_To_Hydrology']**2 + dataset['Vertical_Distance_To_Hydrology']**2)**0.5\n",
    "    dataset_FE['Hydro_plus_Fire'] = dataset['Horizontal_Distance_To_Hydrology'] + dataset['Horizontal_Distance_To_Fire_Points']\n",
    "    dataset_FE['Hydro_minus_Fire'] = dataset['Horizontal_Distance_To_Hydrology'] - dataset['Horizontal_Distance_To_Fire_Points']\n",
    "\n",
    "    dataset_FE['Hydro_plus_Road'] = dataset['Horizontal_Distance_To_Hydrology'] + dataset['Horizontal_Distance_To_Roadways']\n",
    "    dataset_FE['Hydro_minus_Road'] = dataset['Horizontal_Distance_To_Hydrology'] - dataset['Horizontal_Distance_To_Roadways']\n",
    "    dataset_FE['Fire_plus_Road'] = dataset['Horizontal_Distance_To_Fire_Points'] + dataset['Horizontal_Distance_To_Roadways']\n",
    "    dataset_FE['Fire_minus_Road'] = dataset['Horizontal_Distance_To_Fire_Points'] - dataset['Horizontal_Distance_To_Roadways']\n",
    "    return dataset_FE\n",
    "\n",
    "\n",
    "dataset_FE=feature_engineering(dataset)\n",
    "#dataset_FE.head()\n",
    "\n",
    "cols = []\n",
    "cols = list(dataset_FE.columns)\n",
    "cols.remove('Cover_Type')\n",
    "X_train_sub = dataset_FE[cols].values\n",
    "Y_train_sub = dataset_FE['Cover_Type'].values\n",
    "\n",
    "print Y_train_sub\n",
    "\n",
    "best_model_sub.fit(X_train_sub,Y_train_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare testing data\n",
    "#prepare training data\n",
    "dataset_t = pd.read_csv(\"../data_tree/test.csv\")\n",
    "ID = dataset_t['Id']\n",
    "dataset_t.drop(['Id','Soil_Type7', 'Soil_Type15'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "dataset_FE_t=feature_engineering(dataset_t)\n",
    "X_test_sub = dataset_FE_t.values\n",
    "\n",
    "predictions = best_model_sub.predict(X_test_sub)\n",
    "\n",
    "#writing the prediction result for submission\n",
    "with open(\"submission_FE2.csv\", \"w\") as subfile:\n",
    "    subfile.write(\"Id,Cover_Type\\n\")\n",
    "    for i, pred in enumerate(list(predictions)):\n",
    "        subfile.write(\"%s,%s\\n\"%(ID[i],pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature parameter tuning - 3rd submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Create the parameters list you wish to tune\n",
    "\n",
    "parameters = {\n",
    "    'n_estimators': [150],\n",
    "    'max_features': ['auto',0.6,0.8],\n",
    "    'min_samples_split': [2,4,8,16,32,64],\n",
    "    'min_samples_leaf' : [1,10,25,50],\n",
    "    'n_jobs': [-1],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Initialize the classifier\n",
    "best_model_GV = ExtraTreesClassifier()\n",
    "\n",
    "# Make an f1 scoring function using 'make_scorer' \n",
    "f1_scorer = make_scorer(f1_score,average='weighted')\n",
    "\n",
    "\n",
    "# create cross-validation data set and apply GridSearch\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "cv = StratifiedShuffleSplit(Y_train_sub,10,random_state=42)\n",
    "\n",
    "# Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(best_model_GV, parameters, f1_scorer, cv = cv)\n",
    "\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train_sub, Y_train_sub)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare testing data\n",
    "\n",
    "dataset_t = pd.read_csv(\"../data_tree/test.csv\")\n",
    "ID = dataset_t['Id']\n",
    "dataset_t.drop(['Id','Soil_Type7', 'Soil_Type15'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "dataset_FE_t=feature_engineering(dataset_t)\n",
    "X_test_sub = dataset_FE_t.values\n",
    "\n",
    "clf_predictions = clf.predict(X_test_sub)\n",
    "\n",
    "#writing prediction result for subsmission\n",
    "with open(\"submission_clf2.csv\", \"w\") as subfile:\n",
    "    subfile.write(\"Id,Cover_Type\\n\")\n",
    "    for i, pred in enumerate(list(clf_predictions)):\n",
    "        subfile.write(\"%s,%s\\n\"%(ID[i],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 150, 'random_state': 42, 'n_jobs': -1}\n"
     ]
    }
   ],
   "source": [
    "best_estimator_n_estimator_optimized = grid_obj.best_params_\n",
    "print(best_estimator_n_estimator_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_jobs': -1, 'min_samples_leaf': 1, 'n_estimators': 150, 'min_samples_split': 2, 'random_state': 42, 'max_features': 0.6}\n"
     ]
    }
   ],
   "source": [
    "best_estimator_all_optimized = grid_obj.best_params_\n",
    "print(best_estimator_all_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "clf_predictions_OvA=OneVsRestClassifier(ExtraTreesClassifier(n_estimators=150,random_state=42,n_jobs=1)).fit(X_train_sub, Y_train_sub).predict(dataset_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"submission_clf_ova.csv\", \"w\") as subfile:\n",
    "    subfile.write(\"Id,Cover_Type\\n\")\n",
    "    for i, pred in enumerate(list(clf_predictions_OvA)):\n",
    "        subfile.write(\"%s,%s\\n\"%(ID[i],pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-VS-all implmentation - 4th submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####get training data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "#prepare training data\n",
    "dataset = pd.read_csv(\"../data_tree/train.csv\")\n",
    "#ID = dataset['Id']\n",
    "dataset.drop(['Id','Soil_Type7', 'Soil_Type15'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "#add features\n",
    "def feature_engineering(dataset):\n",
    "    dataset_FE = dataset\n",
    "    dataset_FE['Ele_minus_VDtHyd'] = dataset['Elevation'] - dataset['Vertical_Distance_To_Hydrology']\n",
    "    dataset_FE['Ele_plus_VDtHyd'] = dataset['Elevation'] + dataset['Vertical_Distance_To_Hydrology']\n",
    "    dataset_FE['Distance_to_Hydrolody'] = (dataset['Horizontal_Distance_To_Hydrology']**2 + dataset['Vertical_Distance_To_Hydrology']**2)**0.5\n",
    "    dataset_FE['Hydro_plus_Fire'] = dataset['Horizontal_Distance_To_Hydrology'] + dataset['Horizontal_Distance_To_Fire_Points']\n",
    "    dataset_FE['Hydro_minus_Fire'] = dataset['Horizontal_Distance_To_Hydrology'] - dataset['Horizontal_Distance_To_Fire_Points']\n",
    "\n",
    "    dataset_FE['Hydro_plus_Road'] = dataset['Horizontal_Distance_To_Hydrology'] + dataset['Horizontal_Distance_To_Roadways']\n",
    "    dataset_FE['Hydro_minus_Road'] = dataset['Horizontal_Distance_To_Hydrology'] - dataset['Horizontal_Distance_To_Roadways']\n",
    "    dataset_FE['Fire_plus_Road'] = dataset['Horizontal_Distance_To_Fire_Points'] + dataset['Horizontal_Distance_To_Roadways']\n",
    "    dataset_FE['Fire_minus_Road'] = dataset['Horizontal_Distance_To_Fire_Points'] - dataset['Horizontal_Distance_To_Roadways']\n",
    "    return dataset_FE\n",
    "\n",
    "\n",
    "dataset_FE=feature_engineering(dataset)\n",
    "#dataset_FE.head()\n",
    "\n",
    "cols = []\n",
    "cols = list(dataset_FE.columns)\n",
    "cols.remove('Cover_Type')\n",
    "X_train_sub = dataset_FE[cols].values\n",
    "Y_train_sub = dataset_FE['Cover_Type'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####prep testing data\n",
    "\n",
    "dataset_t = pd.read_csv(\"../data_tree/test.csv\")\n",
    "ID = dataset_t['Id']\n",
    "dataset_t.drop(['Id','Soil_Type7', 'Soil_Type15'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "dataset_FE_t=feature_engineering(dataset_t)\n",
    "X_test_sub = dataset_FE_t.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prep training and testing dataset\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_sub, Y_train_sub, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.914038809898\n"
     ]
    }
   ],
   "source": [
    "#create best model with tuned parameters\n",
    "from sklearn.metrics import f1_score\n",
    "best_model_GV_OVA1 = ExtraTreesClassifier(n_estimators=150,random_state=42,n_jobs=-1)\n",
    "#best_model_GV_OVA2 = ExtraTreesClassifier(n_estimators=150,random_state=42,n_jobs=-1,min_samples_leaf=1,min_samples_split=2,max_features=0.6)\n",
    "\n",
    "pred1 = best_model_GV_OVA1.fit(X_train,y_train).predict(X_test)\n",
    "#pred2 = best_model_GV_OVA2.fit(X_train,y_train).predict(X_test)\n",
    "\n",
    "print f1_score(y_test,pred1,average='weighted')\n",
    "#print f1_score(y_test,pred2,average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[186  28   0   0   5   0   8]\n",
      " [ 26 173   4   0   6   1   0]\n",
      " [  0   1 214   4   0  18   0]\n",
      " [  0   0   2 200   0   2   0]\n",
      " [  0   1   4   0 198   0   0]\n",
      " [  0   1   9   4   0 193   0]\n",
      " [  5   0   0   0   0   0 219]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/binluo/anaconda/lib/python2.7/site-packages/matplotlib/tight_layout.py:222: UserWarning: tight_layout : falling back to Agg renderer\n",
      "  warnings.warn(\"tight_layout : falling back to Agg renderer\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.82  0.12  0.    0.    0.02  0.    0.04]\n",
      " [ 0.12  0.82  0.02  0.    0.03  0.    0.  ]\n",
      " [ 0.    0.    0.9   0.02  0.    0.08  0.  ]\n",
      " [ 0.    0.    0.01  0.98  0.    0.01  0.  ]\n",
      " [ 0.    0.    0.02  0.    0.98  0.    0.  ]\n",
      " [ 0.    0.    0.04  0.02  0.    0.93  0.  ]\n",
      " [ 0.02  0.    0.    0.    0.    0.    0.98]]\n"
     ]
    }
   ],
   "source": [
    "#plot confusion matrix to identify the classes was mis-classfied most\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "class_names = [1,2,3,4,5,6,7]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, pred1)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'Ele_minus_VDtHyd', 'Ele_plus_VDtHyd', 'Distance_to_Hydrolody', 'Hydro_plus_Fire', 'Hydro_minus_Fire', 'Hydro_plus_Road', 'Hydro_minus_Road', 'Fire_plus_Road', 'Fire_minus_Road']\n"
     ]
    }
   ],
   "source": [
    "#prep list columns name for later usage\n",
    "cols_OVA = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'Ele_minus_VDtHyd', 'Ele_plus_VDtHyd', 'Distance_to_Hydrolody', 'Hydro_plus_Fire', 'Hydro_minus_Fire', 'Hydro_plus_Road', 'Hydro_minus_Road', 'Fire_plus_Road', 'Fire_minus_Road', 'Cover_Type']\n",
    "cols_OVA = list(cols_OVA)\n",
    "print cols_OVA[0:61]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Type 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#complete dataset to train but binarize the label\n",
    "dataset_OVA = dataset_FE[cols_OVA]\n",
    "df_7 = dataset_OVA\n",
    "df_7.loc[df_7['Cover_Type'] != 7, 'Cover_Type'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cover_Type\n",
      "-1    12960\n",
      " 7     2160\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_7.groupby('Cover_Type').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120\n",
      "(15120, 61)\n"
     ]
    }
   ],
   "source": [
    "#seperate training and testing datasets\n",
    "X_train_OVA_7 = df_7.iloc[:,0:61].values\n",
    "y_train_OVA_7 = df_7.iloc[:,61].values\n",
    "print len(y_train_OVA_7)\n",
    "print X_train_OVA_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_OVA_7 = best_model_GV_OVA1.fit(X_train_OVA_7,y_train_OVA_7).predict(X_test_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred\n",
      "-1    539154\n",
      " 7     26738\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "###store prediction result for class 7\n",
    "Result_OVA= pd.DataFrame(\n",
    "    {'ID': ID.values,\n",
    "     'pred': pred_OVA_7\n",
    "    })\n",
    "print Result_OVA.groupby('pred').size()\n",
    "\n",
    "#writing out prediction data into csv file for feature consolidation\n",
    "with open(\"Result_OVA_7.csv\", \"w\") as subfile:\n",
    "    subfile.write(\"Id,Cover_Type\\n\")\n",
    "    for i, pred in enumerate(list(pred_OVA_7)):\n",
    "        subfile.write(\"%s,%s\\n\"%(ID[i],pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict type 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cover_Type\n",
      "1    2160\n",
      "2    2160\n",
      "3    2160\n",
      "4    2160\n",
      "5    2160\n",
      "6    2160\n",
      "dtype: int64\n",
      "Cover_Type\n",
      "-1    10800\n",
      " 6     2160\n",
      "dtype: int64\n",
      "12960\n",
      "(12960, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/binluo/anaconda/lib/python2.7/site-packages/pandas/core/indexing.py:465: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=-1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to predict class 6, exclude instances with label as class 7\n",
    "\n",
    "dataset_OVA = dataset_FE[cols_OVA]\n",
    "df_6 = dataset_OVA.loc[dataset_OVA['Cover_Type']!=7]\n",
    "print df_6.groupby('Cover_Type').size()\n",
    "\n",
    "#make all other classes (except for class 6) label as -1\n",
    "df_6.loc[df_6['Cover_Type'] != 6, 'Cover_Type'] = -1\n",
    "print df_6.groupby('Cover_Type').size()\n",
    "\n",
    "#create  training and testing data set\n",
    "X_train_OVA_6 = df_6.iloc[:,0:61].values\n",
    "y_train_OVA_6 = df_6.iloc[:,61].values\n",
    "print len(y_train_OVA_6)\n",
    "print X_train_OVA_6.shape\n",
    "\n",
    "###train without instances labeled as 7\n",
    "\n",
    "best_model_GV_OVA1.fit(X_train_OVA_6,y_train_OVA_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(565892,)\n",
      "(565892, 61)\n",
      "(565892, 62)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(539154, 62)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###predict without labeled as 7\n",
    "###exclude data labeled as 7 out from the testing\n",
    "X_test_sub_OVA = dataset_FE_t[cols_OVA[0:61]]\n",
    "X_test_sub_OVA['ID']=pd.DataFrame({'ID':ID})\n",
    "print(ID.shape)\n",
    "print(dataset_FE_t[cols_OVA[0:61]].shape)\n",
    "print(X_test_sub_OVA.shape)\n",
    "\n",
    "excl_list = Result_OVA.loc[Result_OVA['pred']!=7,'ID']\n",
    "excl_list = list(excl_list.values)\n",
    "X_test_sub_OVA = X_test_sub_OVA[X_test_sub_OVA['ID'].isin(excl_list)]\n",
    "X_test_sub_OVA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predict type 6\n",
    "pred_OVA_6 = best_model_GV_OVA1.predict(X_test_sub_OVA[cols_OVA[0:61]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred\n",
      "-1    520134\n",
      " 6     19020\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##store the prediction result into a dataframe\n",
    "Result_OVA_6= pd.DataFrame(\n",
    "    {'ID': X_test_sub_OVA['ID'].values,\n",
    "     'pred': pred_OVA_6\n",
    "    })\n",
    "print Result_OVA_6.groupby('pred').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred\n",
      "-1    520134\n",
      " 6     19020\n",
      "dtype: int64\n",
      "(539154,)\n",
      "539154\n"
     ]
    }
   ],
   "source": [
    "### store predictionr esult\n",
    "Result_OVA_6= pd.DataFrame(\n",
    "    {'ID': X_test_sub_OVA['ID'].values,\n",
    "     'pred': pred_OVA_6\n",
    "    })\n",
    "print Result_OVA_6.groupby('pred').size()\n",
    "\n",
    "ID_6 = X_test_sub_OVA['ID'].values\n",
    "\n",
    "print(ID_6.shape)\n",
    "\n",
    "print(len(pred_OVA_6))\n",
    "\n",
    "#write the prediction results out to a csv file for future consolidation\n",
    "with open(\"Result_OVA_6.csv\", \"w\") as subfile:\n",
    "    subfile.write(\"Id,Cover_Type\\n\")\n",
    "    for i, pred in enumerate(list(pred_OVA_6)):\n",
    "        subfile.write(\"%s,%s\\n\"%(ID_6[i],pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for type 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cover_Type\n",
      "1    2160\n",
      "2    2160\n",
      "3    2160\n",
      "4    2160\n",
      "5    2160\n",
      "dtype: int64\n",
      "Cover_Type\n",
      "-1    8640\n",
      " 5    2160\n",
      "dtype: int64\n",
      "10800\n",
      "(10800, 61)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=-1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####repeate the process - train the model for type 5 excluding type 7 and 6 in the training data\n",
    "dataset_OVA = dataset_FE[cols_OVA]\n",
    "df_5 = dataset_OVA.loc[dataset_OVA['Cover_Type']!=7]\n",
    "df_5 = df_5.loc[df_5['Cover_Type']!=6]\n",
    "print df_5.groupby('Cover_Type').size()\n",
    "\n",
    "###make other classes to be -1 lable\n",
    "df_5.loc[df_5['Cover_Type'] != 5, 'Cover_Type'] = -1\n",
    "print df_5.groupby('Cover_Type').size()\n",
    "\n",
    "X_train_OVA_5 = df_5.iloc[:,0:61].values\n",
    "y_train_OVA_5 = df_5.iloc[:,61].values\n",
    "print len(y_train_OVA_5)\n",
    "print X_train_OVA_5.shape\n",
    "\n",
    "###train without instances labeled as 7 and 6\n",
    "best_model_GV_OVA5 = ExtraTreesClassifier(n_estimators=150,random_state=42,n_jobs=-1)\n",
    "best_model_GV_OVA5.fit(X_train_OVA_5,y_train_OVA_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred\n",
      "-1    499667\n",
      " 5     20467\n",
      "dtype: int64\n",
      "(520134,)\n",
      "520134\n"
     ]
    }
   ],
   "source": [
    "#predict without label 7 and 6\n",
    "###exclude data labeled as 7 out from the testing\n",
    "X_test_sub_OVA_5 = dataset_FE_t[cols_OVA[0:61]]\n",
    "X_test_sub_OVA_5['ID']=pd.DataFrame({'ID':ID})\n",
    "\n",
    "excl_list = Result_OVA.loc[Result_OVA['pred']!=7,'ID']\n",
    "excl_list = list(excl_list.values)\n",
    "tempset=set(excl_list) - set(list(Result_OVA_6.loc[Result_OVA_6['pred']==6,'ID'].values))\n",
    "\n",
    "X_test_sub_OVA_5 = X_test_sub_OVA_5[X_test_sub_OVA_5['ID'].isin(list(tempset))]\n",
    "X_test_sub_OVA_5.shape\n",
    "\n",
    "#make prediction\n",
    "pred_OVA_5 = best_model_GV_OVA5.predict(X_test_sub_OVA_5[cols_OVA[0:61]])\n",
    "\n",
    "Result_OVA_5= pd.DataFrame(\n",
    "    {'ID': X_test_sub_OVA_5['ID'].values,\n",
    "     'pred': pred_OVA_5\n",
    "    })\n",
    "print Result_OVA_5.groupby('pred').size()\n",
    "\n",
    "ID_5 = X_test_sub_OVA_5['ID'].values\n",
    "\n",
    "print(ID_5.shape)\n",
    "\n",
    "print(len(pred_OVA_5))\n",
    "\n",
    "#write the prediction results out to a csv file for future consolidation\n",
    "with open(\"Result_OVA_5.csv\", \"w\") as subfile:\n",
    "    subfile.write(\"Id,Cover_Type\\n\")\n",
    "    for i, pred in enumerate(list(pred_OVA_5)):\n",
    "        subfile.write(\"%s,%s\\n\"%(ID_5[i],pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict type 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cover_Type\n",
      "1    2160\n",
      "2    2160\n",
      "3    2160\n",
      "4    2160\n",
      "dtype: int64\n",
      "Cover_Type\n",
      "-1    6480\n",
      " 4    2160\n",
      "dtype: int64\n",
      "8640\n",
      "(8640, 61)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=-1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####train the model\n",
    "dataset_OVA = dataset_FE[cols_OVA]\n",
    "df_4 = dataset_OVA.loc[dataset_OVA['Cover_Type']!=7]\n",
    "df_4 = df_4.loc[df_4['Cover_Type']!=6]\n",
    "df_4 = df_4.loc[df_4['Cover_Type']!=5]\n",
    "print df_4.groupby('Cover_Type').size()\n",
    "\n",
    "df_4.loc[df_4['Cover_Type'] != 4, 'Cover_Type'] = -1\n",
    "print df_4.groupby('Cover_Type').size()\n",
    "\n",
    "X_train_OVA_4 = df_4.iloc[:,0:61].values\n",
    "y_train_OVA_4 = df_4.iloc[:,61].values\n",
    "print len(y_train_OVA_4)\n",
    "print X_train_OVA_4.shape\n",
    "\n",
    "###train without instances labeled as 7\n",
    "best_model_GV_OVA4 = ExtraTreesClassifier(n_estimators=150,random_state=42,n_jobs=-1)\n",
    "best_model_GV_OVA4.fit(X_train_OVA_4,y_train_OVA_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(499667, 62)\n",
      "pred\n",
      "-1    497686\n",
      " 4      1981\n",
      "dtype: int64\n",
      "(499667,)\n",
      "499667\n"
     ]
    }
   ],
   "source": [
    "#predict without label 7,6 and 5\n",
    "###exclude data labeled as 7,6 and 5 out from the testing\n",
    "X_test_sub_OVA_4 = dataset_FE_t[cols_OVA[0:61]]\n",
    "X_test_sub_OVA_4['ID']=pd.DataFrame({'ID':ID})\n",
    "\n",
    "excl_list = Result_OVA.loc[Result_OVA['pred']!=7,'ID']\n",
    "excl_list = list(excl_list.values)\n",
    "tempset=set(excl_list) - set(list(Result_OVA_6.loc[Result_OVA_6['pred']==6,'ID'].values))\n",
    "tempset=tempset - set(list(Result_OVA_5.loc[Result_OVA_5['pred']==5,'ID'].values))\n",
    "\n",
    "X_test_sub_OVA_4 = X_test_sub_OVA_4[X_test_sub_OVA_4['ID'].isin(list(tempset))]\n",
    "print(X_test_sub_OVA_4.shape)\n",
    "\n",
    "#make prediction\n",
    "pred_OVA_4 = best_model_GV_OVA4.predict(X_test_sub_OVA_4[cols_OVA[0:61]])\n",
    "\n",
    "Result_OVA_4= pd.DataFrame(\n",
    "    {'ID': X_test_sub_OVA_4['ID'].values,\n",
    "     'pred': pred_OVA_4\n",
    "    })\n",
    "print Result_OVA_4.groupby('pred').size()\n",
    "\n",
    "ID_4 = X_test_sub_OVA_4['ID'].values\n",
    "\n",
    "print(ID_4.shape)\n",
    "\n",
    "print(len(pred_OVA_4))\n",
    "\n",
    "#write the prediction results out to a csv file for future consolidation\n",
    "with open(\"Result_OVA_4.csv\", \"w\") as subfile:\n",
    "    subfile.write(\"Id,Cover_Type\\n\")\n",
    "    for i, pred in enumerate(list(pred_OVA_4)):\n",
    "        subfile.write(\"%s,%s\\n\"%(ID_4[i],pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict type 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cover_Type\n",
      "1    2160\n",
      "2    2160\n",
      "3    2160\n",
      "dtype: int64\n",
      "6480\n",
      "(6480, 61)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=-1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####train the model with data excluding type 4,5, 6,7\n",
    "dataset_OVA = dataset_FE[cols_OVA]\n",
    "df_123 = dataset_OVA.loc[dataset_OVA['Cover_Type']!=7]\n",
    "df_123 = df_123.loc[df_123['Cover_Type']!=6]\n",
    "df_123 = df_123.loc[df_123['Cover_Type']!=5]\n",
    "df_123 = df_123.loc[df_123['Cover_Type']!=4]\n",
    "print df_123.groupby('Cover_Type').size()\n",
    "\n",
    "X_train_OVA_123 = df_123.iloc[:,0:61].values\n",
    "y_train_OVA_123 = df_123.iloc[:,61].values\n",
    "print len(y_train_OVA_123)\n",
    "print X_train_OVA_123.shape\n",
    "\n",
    "###train without instances labeled as 7, 6,5,4\n",
    "best_model_GV_OVA123 = ExtraTreesClassifier(n_estimators=150,random_state=42,n_jobs=-1)\n",
    "best_model_GV_OVA123.fit(X_train_OVA_123,y_train_OVA_123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(497686, 62)\n",
      "pred\n",
      "1    212851\n",
      "2    246188\n",
      "3     38647\n",
      "dtype: int64\n",
      "(497686,)\n",
      "497686\n"
     ]
    }
   ],
   "source": [
    "#predict without label 7 and 6, 5,4\n",
    "###exclude data labeled as 7,6,5,4 out from the testing\n",
    "X_test_sub_OVA_123 = dataset_FE_t[cols_OVA[0:61]]\n",
    "X_test_sub_OVA_123['ID']=pd.DataFrame({'ID':ID})\n",
    "\n",
    "excl_list = Result_OVA.loc[Result_OVA['pred']!=7,'ID']\n",
    "excl_list = list(excl_list.values)\n",
    "tempset=set(excl_list) - set(list(Result_OVA_6.loc[Result_OVA_6['pred']==6,'ID'].values))\n",
    "tempset=tempset - set(list(Result_OVA_5.loc[Result_OVA_5['pred']==5,'ID'].values))\n",
    "tempset=tempset - set(list(Result_OVA_4.loc[Result_OVA_4['pred']==4,'ID'].values))\n",
    "\n",
    "X_test_sub_OVA_123 = X_test_sub_OVA_123[X_test_sub_OVA_123['ID'].isin(list(tempset))]\n",
    "print(X_test_sub_OVA_123.shape)\n",
    "\n",
    "#make prediction\n",
    "pred_OVA_123 = best_model_GV_OVA123.predict(X_test_sub_OVA_123[cols_OVA[0:61]])\n",
    "\n",
    "Result_OVA_123= pd.DataFrame(\n",
    "    {'ID': X_test_sub_OVA_123['ID'].values,\n",
    "     'pred': pred_OVA_123\n",
    "    })\n",
    "print Result_OVA_123.groupby('pred').size()\n",
    "\n",
    "ID_123 = X_test_sub_OVA_123['ID'].values\n",
    "\n",
    "print(ID_123.shape)\n",
    "\n",
    "print(len(pred_OVA_123))\n",
    "\n",
    "with open(\"Result_OVA_123.csv\", \"w\") as subfile:\n",
    "    subfile.write(\"Id,Cover_Type\\n\")\n",
    "    for i, pred in enumerate(list(pred_OVA_123)):\n",
    "        subfile.write(\"%s,%s\\n\"%(ID_123[i],pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565892, 2)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [Result_OVA_123, Result_OVA_4.loc[Result_OVA_4['pred']==4],Result_OVA_5.loc[Result_OVA_5['pred']==5],Result_OVA_6.loc[Result_OVA_6['pred']==6],Result_OVA.loc[Result_OVA['pred']==7]]\n",
    "\n",
    "Result_OVA_ALL = pd.concat(frames)\n",
    "\n",
    "Result_OVA_ALL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write data out for submission\n",
    "with open(\"submission_OVA_all.csv\", \"w\") as subfile:\n",
    "    subfile.write(\"Id,Cover_Type\\n\")\n",
    "    for index, row in Result_OVA_ALL.iterrows():\n",
    "        subfile.write(\"%s,%s\\n\"%(row['ID'],row['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred\n",
       "1    212851\n",
       "2    246188\n",
       "3     38647\n",
       "4      1981\n",
       "5     20467\n",
       "6     19020\n",
       "7     26738\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result_OVA_ALL.groupby('pred').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
